---
title: "Homework 6"
author: "CS 498, Spring 2018, Xiaoming Ji"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
---


#Problem 1
Linear regression with various regularizers The UCI Machine Learning dataset repository hosts a dataset giving features of music, and the latitude and longitude from which that music originates here. Investigate methods to predict latitude and longitude from these features, as below. **There are actually two versions of this dataset. Either one is OK by me, we will use the one with more independent variables . We will ignore outliers. We also regard latitude and longitude as entirely independent.

```{r, message=FALSE, warning=FALSE}
library(readr)

COL_NUM = 118

ALL_DF = read_csv("./Geographical Original of Music/default_plus_chromatic_features_1059_tracks.txt", 
                  col_names = sapply(1:COL_NUM, FUN =function(x) {paste("F", x, sep = "")}))
colnames(ALL_DF)[COL_NUM - 1] = "Lat"
colnames(ALL_DF)[COL_NUM] = "Long"

#Change origin in order to eliminate negative value
ALL_DF$Lat = 180 + ALL_DF$Lat
ALL_DF$Long =  180 + ALL_DF$Long

Lat_DF = ALL_DF[, -COL_NUM]
Long_DF = ALL_DF[, -(COL_NUM - 1)]
```

## 1.1
First, build a straightforward linear regression of latitude (resp. longitude) against features. What is the R-squared? Plot a graph evaluating each regression.

```{r}
lat_model = lm(Lat ~ ., data = Lat_DF)
long_model = lm(Long ~ ., data = Long_DF)
```

We get R-squred for each model as,

- Latitude: $`r summary(lat_model)$r.squared`$
- Longitude: $`r summary(long_model)$r.squared`$

Plots of Residual vs Fitted Values as,

- Latitude:
```{r, echo=FALSE}
plot_residule = function (values, residuals, xtitle, ytitle){
  plot(values, residuals, col = "dodgerblue",
  pch = 20, cex = 1.5, xlab = xtitle, ylab = ytitle)
  abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
}

plot_residule(lat_model$fitted.values, lat_model$residuals, 
              "Latitude Fitted values", "Latitude Residuals")
```
- Longitude:
```{r, echo=FALSE}
plot_residule(long_model$fitted.values, long_model$residuals, 
              "Longitude Fitted values", "Longitude Residuals")
```

## 1.2
Does a Box-Cox transformation improve the regressions? 

We make boxcox plot to determine $\lambda$.
```{r}
library(MASS)
par(mfrow=c(1, 2))
boxcox(lat_model, lambda = seq(1, 12, 0.1), plotit = TRUE)
boxcox(long_model, lambda = seq(0, 2, 0.1), plotit = TRUE)
```

From the above plots, we see the best $\lambda$,

- Latitude: 6
- Longitude: 1, which means no need to do transformation.

We evaluate whether Box-Cox transformation for $\lambda=6$ can give us better Latitude regression model.

We firstly make Residual vs Fitted Values plot.

```{r}
boxcox_lat_model = lm((Lat ^ 6 - 1) / 6 ~ ., data = Lat_DF)

v = (6 * boxcox_lat_model$fitted.values + 1) ^ (1/6)
r = Lat_DF$Lat - v

plot_residule(v, r, "Box-Cox Latitude Fitted values", 
              "Box-Cox Latitude Fitted values")
```

This plot dosen't seem to have much different with the regular one. We then check the R Squared, R Adujusted Squared and RMSE value of both models.

```{r}
regular_rmse = sqrt(mean(resid(lat_model) ^ 2))
boxcox_rmse = sqrt(mean(r ^ 2))
```

|Model|R.Squared|R.Adujusted.Squared|RMSE|
|-----|---------|------------------|-----|
|Regular|$`r summary(lat_model)["r.squared"]`$|$`r summary(lat_model)["adj.r.squared"]`$|$`r regular_rmse`$|
|Box-Cox|$`r summary(boxcox_lat_model)["r.squared"]`$|$`r summary(boxcox_lat_model)["adj.r.squared"]`$|$`r boxcox_rmse`$|

Above info shows Box-Cox Latitude regression model has better R Squared (12.7% gain) and R Adujusted Squared (16.6% gain) value than the regular one. Although it has a bigger RMSE, the increasement is relatively small (3.6% bigger). Thus, we believe the Box-Cox model is better.

**To conclude:**

- Box-Cox transformation gives better Latitude regression model. 
- We will not use any transformation for Longitude model.


## 1.3
Use glmnet to produce:

- A regression regularized by L2 (equivalently, a ridge regression). You should estimate the regularization coefficient that produces the minimum error. Is the regularized regression better than the unregularized regression?
- A regression regularized by L1 (equivalently, a lasso regression). You should estimate the regularization coefficient that produces the minimum error. How many variables are used by this regression? Is the regularized regression better than the unregularized regression?
- A regression regularized by elastic net (equivalently, a regression regularized by a convex combination of L1 and L2). Try three values of alpha, the weight setting how big L1 and L2 are. You should estimate the regularization coefficient that produces the minimum error. How many variables are used by this regression? Is the regularized regression better than the unregularized regression?


#Problem 2
Logistic regression The UCI Machine Learning dataset repository hosts a dataset giving whether a Taiwanese credit card user defaults against a variety of features here. Use logistic regression to predict whether the user defaults. You should ignore outliers, but you should try the various regularization schemes we have discussed.
