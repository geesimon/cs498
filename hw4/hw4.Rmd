---
title: "Homework 1"
author: "CS 498, Spring 2018, Xiaoming Ji"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
---


#Problem 1

You can find a dataset dealing with European employment in 1979 at http://lib.stat.cmu.edu/DASL/Stories/EuropeanJobs.html. This dataset gives the percentage of people employed in each of a set of areas in 1979 for each of a set of European countries. Notice this dataset contains only 26 data points.

##Part (1)
Use an agglomerative clusterer to cluster this data. Produce a dendrogram of this data for each of single link, complete link, and group average clustering. 

```{r, message=FALSE}
library(readr)
EuropeanJobs = read_csv("EuropeanJobs.csv")
Countries = EuropeanJobs$Country
JobsStat = EuropeanJobs[, -1]
d = dist(JobsStat)
```
```{r}
plot(hclust(d, method = "single"), labels = Countries, main = "Single Link Clustering", xlab = "")
```

- This clustering expose **Turkey** as a very special single node cluster against every other countries. It makes some sense as Turkey is a country span both Asia and Europe.

```{r}
plot(hclust(d, method = "complete"), labels = Countries, main = "Complete Link Clustering", xlab = "")
```

- From some height, this clustering makes Turkey, Greece and Yugoslavia one cluster, Rumania, Czechoslovakia, USSR etc a second cluster and the rest counties a 3rd cluster If we check the history, countries in the second cluster are most socialism countries at that time (except Spain, Portugal and Ireland) and also close to each other. Countries in the 3rd cluster are  capitalism countries except East Germany which is closely grouped with West Germany (since they were one country). 
- Countries close to each other will more likely to be clustered together. For example: East Germany and West Germany. Netherlands, Norway, Denmark and Sweden. Belgium and United Kingdom.

```{r}
plot(hclust(d, method = "average"), labels = Countries, main = "Group Average Clustering", xlab = "")
```

- The results of this clustering is very similar to the Complete Link Clustering.

##Part (2)
Using k-means, cluster this dataset. determine what is a good choice of k for this data.

We make plot of the cost (total within-cluster sum of squares) for different values of k. 

```{r}
ws = lapply(1:(nrow(JobsStat) - 1), function(x){kmeans(JobsStat, centers = x)$tot.withinss})

plot(1:(nrow(JobsStat) - 1), ws, type="o", xlab = "Number of Clusters", ylab = "Within groups sum of squares", pch = 20)
```
Notice there is a sharp drop in cost from k = 1 to k = 3. After that, the cost falls off slowly. This suggests using k = 3 or k = 4 are the good choices for k.

#Problem 2
Obtain the activities of daily life dataset from the UC Irvine machine learning
website (https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer
data provided by Barbara Bruno, Fulvio Mastrogiovanni and Antonio Sgor-
bissa).

Let's load all data to a data frame.
```{r}
ActivityLabels = c("Brush_teeth", "Climb_stairs", "Comb_hair", "Descend_stairs",
                   "Drink_glass", "Eat_meat", "Eat_soup", "Getup_bed", 
                   "Liedown_bed", "Pour_water", "Sitdown_chair", 
                   "Standup_chair", "Use_telephone", "Walk")
DataFolderName = "HMP_Dataset"
AllData = list()

for (i in 1:length(ActivityLabels)){
  label = ActivityLabels[i]
  folder_name = paste("./HMP_Dataset/", label, sep = "")
  file_names = dir(folder_name)
  for (j in 1:length(file_names)){
    full_file_name = paste(folder_name, "/", file_names[j], sep = "")
    data =  read_delim(full_file_name, " ", col_names = c("X", "Y", "Z"), 
                       col_types = cols(X="i", Y="i", Z="i"), trim_ws = TRUE)
    if(j == 1) {
      AllData[[label]] = data
    } else{
       AllData[[label]] = rbind(AllData[[label]], data)
    }
  }
}
```


##Part (a)
Build a classifier that classifies sequences into one of the 14 activities provided. To make features, you should vector quantize, then use a histogram
of cluster centers (as described in the subsection; this gives a pretty explicit set of steps to follow). You will find it helpful to use hierarchical
k-means to vector quantize. You may use whatever multi-class classifier
you wish, though I’d start with R’s decision forest, because it’s easy to
use and effective. You should report (a) the total error rate and (b) the
class confusion matrix of your classifier.

##Part (b)
Now see if you can improve your classifier by (a) modifying the number
of cluster centers in your hierarchical k-means and (b) modifying the size
of the fixed length samples that you use.