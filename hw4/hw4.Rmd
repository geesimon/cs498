---
title: "Homework 1"
author: "CS 498, Spring 2018, Xiaoming Ji"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
params:
  load_precomputed_data: no
---


#Problem 1

You can find a dataset dealing with European employment in 1979 at http://lib.stat.cmu.edu/DASL/Stories/EuropeanJobs.html. This dataset gives the percentage of people employed in each of a set of areas in 1979 for each of a set of European countries. Notice this dataset contains only 26 data points.

##Part (1)
Use an agglomerative clusterer to cluster this data. Produce a dendrogram of this data for each of single link, complete link, and group average clustering. 

```{r, message=FALSE}
library(readr)
EuropeanJobs = read_csv("EuropeanJobs.csv")
Countries = EuropeanJobs$Country
JobsStat = EuropeanJobs[, -1]
d = dist(JobsStat)
```
```{r}
plot(hclust(d, method = "single"), labels = Countries, main = "Single Link Clustering", xlab = "")
```

- This clustering expose **Turkey** as a very special single node cluster against every other countries. It makes some sense as Turkey is a country span both Asia and Europe.

```{r}
plot(hclust(d, method = "complete"), labels = Countries, main = "Complete Link Clustering", xlab = "")
```

- From some height, this clustering makes Turkey, Greece and Yugoslavia one cluster, Rumania, Czechoslovakia, USSR etc a second cluster and the rest counties a 3rd cluster If we check the history, countries in the second cluster are most socialism countries at that time (except Spain, Portugal and Ireland) and also close to each other. Countries in the 3rd cluster are  capitalism countries except East Germany which is closely grouped with West Germany (since they were one country). 
- Countries close to each other will more likely to be clustered together. For example: East Germany and West Germany. Netherlands, Norway, Denmark and Sweden. Belgium and United Kingdom.

```{r}
plot(hclust(d, method = "average"), labels = Countries, main = "Group Average Clustering", xlab = "")
```

- The results of this clustering is very similar to the Complete Link Clustering.

##Part (2)
Using k-means, cluster this dataset. determine what is a good choice of k for this data.

We make plot of the cost (total within-cluster sum of squares) for different values of k. 

```{r}
set.seed(1972)
ws = lapply(1:(nrow(JobsStat) - 1), function(x){kmeans(JobsStat, centers = x)$tot.withinss})

plot(1:(nrow(JobsStat) - 1), ws, type="o", xlab = "Number of Clusters", ylab = "Within groups sum of squares", pch = 20)
```
Notice there is a sharp drop in cost from k = 1 to k = 4. After that, the cost falls off slowly. This suggests using k = 4 or k = 5 are the good choices for k. The plots from part (1) support this conclusion.

#Problem 2
Obtain the activities of daily life dataset from the UC Irvine machine learning
website (https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer
data provided by Barbara Bruno, Fulvio Mastrogiovanni and Antonio Sgor-
bissa).

Let's load all data and split them among training and test dataset.
```{r}
ActivityLabels = c("Brush_teeth", "Climb_stairs", "Comb_hair", "Descend_stairs",
                   "Drink_glass", "Eat_meat", "Eat_soup", "Getup_bed", 
                   "Liedown_bed", "Pour_water", "Sitdown_chair", 
                   "Standup_chair", "Use_telephone", "Walk")
DataFolderName = "HMP_Dataset"
AllRawData = list()
RawTrainData = list()
RawTestData = list()

for (i in 1:length(ActivityLabels)){
  activity_data = list()
  label = ActivityLabels[i]
  folder_name = paste("./HMP_Dataset/", label, sep = "")
  file_names = dir(folder_name)
  for (j in 1:length(file_names)){
    full_file_name = paste(folder_name, "/", file_names[j], sep = "")
    data =  read_delim(full_file_name, " ", col_names = c("X", "Y", "Z"),
                       col_types = cols(X="i", Y="i", Z="i"), trim_ws = TRUE)
    activity_data[[j]] = data
  }
  AllRawData[[i]] = activity_data
}

set.seed(19720816)
#Sample 80% as training data and the rest 20% as test data. Save them to RawTrainData and RawTestData
for (i in 1:length(ActivityLabels)){
  all_indexes = 1:(length(AllRawData[[i]]))
  train_indexes = sample(all_indexes, floor((length(AllRawData[[i]])) * 0.8))
  #train_indexes = 1:(floor((length(AllRawData[[i]])) * 0.8))
  test_indexes = all_indexes[-train_indexes]
  train_data = list()
  test_data = list()
  for(j in 1:length(train_indexes)){
    train_data[[j]] = AllRawData[[i]][[train_indexes[j]]]
  }
  for(j in 1:length(test_indexes)){
    test_data[[j]] = AllRawData[[i]][[test_indexes[j]]]
  }
  RawTrainData[[i]] = train_data
  RawTestData[[i]] = test_data
}
```


```{r}
#Cut nx3 dataframe as samples vectors
cut.as.vector = function(df3, sample_length){
  row_count = floor(dim(df3)[1] / sample_length)
  m_data = matrix(0, row_count, sample_length * 3)
  
  index = 1
  for(i in 1:row_count){
    d = df3[index:(index + sample_length -1),]
    d = as.matrix(d)
    dim(d) = c(sample_length * 3, 1)
    d = as.vector(d)
    m_data[i,] = d
    
    index = index + sample_length
  }
  
  return (m_data)
}

# Cut activity data as fixed size data
cut.listdf = function(ll_df, sample_length){
  new_ll_df = list()

  for (i in 1:length(ll_df)){
    activity_data = list()
    for (j in 1:length(ll_df[[i]])){
      activity_data[[j]] = cut.as.vector(ll_df[[i]][[j]], sample_length)
    }
    new_ll_df[[i]] = activity_data
  }
  
  return (new_ll_df)
}
```

#Cut data to fix-size vector (32*3)
```{r}
TrainData = cut.listdf(RawTrainData, 32)
TestData = cut.listdf(RawTestData, 32)
```


##Part (a)
Build a classifier that classifies sequences into one of the 14 activities provided. To make features, you should vector quantize, then use a histogram
of cluster centers (as described in the subsection; this gives a pretty explicit set of steps to follow). You will find it helpful to use hierarchical
k-means to vector quantize. You may use whatever multi-class classifier
you wish, though I’d start with R’s decision forest, because it’s easy to
use and effective. You should report (a) the total error rate and (b) the
class confusion matrix of your classifier.

We build a dictionary by clustering the training data with $$k=480$$.
```{r}
#Merge all training data
all_data = data.frame()
for (i in 1:length(TrainData)){
  for (j in 1:length(TrainData[[i]])){
    all_data = rbind(all_data, TrainData[[i]][[j]])
  }
}

set.seed(1972)
km = kmeans(all_data, centers = 480)
```

We vector quantize training and test dataset and compute the histgram of these dataset as features.
```{r}
vector_quantize = function(df, centers){
  df = as.matrix(df)
  centers = as.matrix(centers)
  cv = rep(0, dim(df)[1])
  
  for (i in 1:(dim(df)[1])){
    c = t(centers) - df[i,]
    d = colSums(c^2)
    cv[i] = which.min(d)
  }
  return (cv)
}

#Compute histgram of the data and assign label number. This will make feature vectors for classification.
featurize_data = function(ll_df, centers){
  all_features = data.frame()
  sample_interval = seq(0, dim(centers)[1], 1)

  for (i in 1:length(ll_df)){
    df = data.frame()

    mv = matrix(0, length(ll_df[[i]]), length(sample_interval) - 1)
    for (j in 1:length(ll_df[[i]])){
      mv[j,] = hist(vector_quantize(ll_df[[i]][[j]], centers), breaks = sample_interval, 
                          plot = FALSE)$density
    }
    all_features = rbind(all_features, cbind(as.data.frame(mv),label=i))
  }
  
  return (all_features)
}

HistTrainData = featurize_data(TrainData, km$centers)
HistTestData = featurize_data(TestData, km$centers)
```

```{r eval=!params$load_precomputed_data}
library(h2o)

set.seed(19720816)
get_best_classifier = function(train_frame, test_frame, ntrees, max_depths) {
  accuracies = rep(0, length(ntrees) * length(max_depths))
  models = list()
  
  index = 1
  for (ntree in 1:length(ntrees)) {
    for (depth in 1:length(max_depths)) {
      rf = h2o.randomForest(y = "label", training_frame = train_frame, ntrees = ntrees[ntree], 
                            max_depth = max_depths[depth])
      prediction = h2o.predict(rf, newdata = test_frame[-dim(test_frame)[2]])
      
      models[[index]] = rf
      accuracies[index] = sum(prediction[,1] == test_frame["label"]) / dim(test_frame)[1]
      index = index + 1
    }
  }
  
  accuracy = max(accuracies)
  model = models[[which.max(accuracies)]]
  
  return (list(accuracy = accuracy, model = model))
}

h2o.init(nthreads=-1, max_mem_size = "2G")
h2o.removeAll()

HistTrainData$label = as.factor(ActivityLabels[HistTrainData$label])
HistTestData$label = as.factor(ActivityLabels[HistTestData$label])

h2o_train_data = as.h2o(HistTrainData)
h2o_test_data = as.h2o(HistTestData)

result = get_best_classifier(h2o_train_data, h2o_test_data, c(10, 20, 30), c(4, 8, 16))
result[["CM"]] = h2o.confusionMatrix(result$model)

save(result, file="result.data")
h2o.shutdown(prompt=FALSE)
```

```{r}
#load("result.data")

error_rate = 1 - result[["accuracy"]]
```

- Error rate: 

- Confusion Matrix:
$`r result[["CM"]]`$

##Part (b)
Now see if you can improve your classifier by (a) modifying the number
of cluster centers in your hierarchical k-means and (b) modifying the size
of the fixed length samples that you use.


```{r}

```

