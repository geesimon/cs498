```{r}
topic_EM = function(docs, vocab.size, topic.count, threshold = 1e-7){
  #Initialize P and Pi
  doc_count = dim(docs)[1]
  doc_length = rowSums(docs)
  P  = matrix(0, topic.count, vocab.size)
  pi = rep(0, topic.count)
  old_t_W  = matrix(0, topic.count, doc_count)
  
  #Randomly sample docs
  i_doc = sample(1:doc_count, doc_count)
  
  bat_size = round(doc_count / topic.count)
  s = 1
  e = bat_size
  
  for (j in 1:topic.count) {
    if (e > doc_count) e = doc_count
    
    i_bat = i_doc[s:e]
    
    #count words and add smoothing
    words = colSums(docs[i_bat,])
    
    #Calculate P and Pi
    P[j,]  = words / sum(words)
    pi[j] = length(i_bat) / doc_count
    
    s = e + 1
    e = e + bat_size
  }

  #Iterately perform E and M steps
  for (i in 1:1000) {
    #smooth P
    P = P + 1e-7
    P = P / rowSums(P)
    
    log_P = log(P)
    log_pi = log(pi)
    
    #Calculate Q (for testing only)
    W = t(old_t_W)
    X_log_P = docs %*% t(log(P))
    Q = 0
    for (i in 1:doc_count) {
      for (j in 1:topic.count){
        Q = Q + (X_log_P[i,j] + log_pi[j]) * W[i,j]
      }
    }
    print(paste("Q=",Q))
    
    #E Step
    X_log_P = docs %*% t(log_P)
    t_log_W = apply(X_log_P, 1, FUN=function(x_log_p){
      p_j = x_log_p + log_pi
      return (p_j - logSumExp(p_j))
      })
    
    t_W = exp(t_log_W)
    
    d = max(abs(t_W - old_t_W))
    print(d)
    if(d < threshold) break()
    
    old_t_W = t_W
    
    #M Step
    P = (t_W %*% docs) / c(t_W %*% doc_length)
    pi = rowSums(t_W) / doc_count
  }
  
  return (list(W=t(old_t_W), P=P, pi=pi))
}
```

```{r}
GMM_EM = function(X, cluster.count, converge.threshold = 1e-7) {
  N = dim(X)[1]
  #Take kmean as the initial value for U and Pi
  kmeans_cluster = kmeans(X, cluster.count)
  
  U = kmeans_cluster$centers
  pi = rep(0, cluster.count) 
  for (i in 1:cluster.count){
    pi[i] = sum(kmeans_cluster$cluster == i) / N
  }
  
  W = matrix(0, N, cluster.count)
  old_W = W
  
  #Need use logSumExp
  
  for (iter in 1:10000) {
    #E Step
    for (j in 1:cluster.count) {
      D = t(X) - U[j]
      W[,j] = exp(-0.5 * colSums(D ^ 2)) * pi[j]
    }
    W = W / rowSums(W)
    
    d = max(abs(W - old_W))
    print(paste("Iteration=", iter, ", D=", d, sep=""))
    if(d < converge.threshold) break()
    
    old_W = W
    
    #M Step
    w_sum = colSums(W)
    U = crossprod(W, X) / w_sum
    pi = w_sum / N
  }
  
  return (list(centers = U, pi=pi))
}
```

```{r}
GMM_EM = function(X, cluster.count, converge.threshold = 1e-7) {
  N = dim(X)[1]
  W = matrix(0, N, cluster.count)
  #Take kmean as the initial value for U and Pi
  kmeans_cluster = kmeans(X, cluster.count)
  
  U = kmeans_cluster$centers
  pi = kmeans_cluster$size / N
  for (j in 1:cluster.count){
    W[kmeans_cluster$cluster == j, j] = 1
  }
  
  #return (list(W = W, centers = kmeans_cluster$centers, pi=pi))
  
  old_W = W
  log_W = W

  for (iter in 1:10000) {
    log_pi = log(pi)
    #E Step
    # for (j in 1:cluster.count) {
    #   D = X - U[j]
    #   log_W[,j] = (-0.5 * rowSums(D ^ 2)) + log_pi[j]
    # }
    # for (i in 1:N) {
    #   log_W[i, ] = log_W[i,] - logSumExp(log_W[i,])
    # }
    for (i in 1:N) {
      for (j in 1:cluster.count){
        d_ij = X[i,] - U[j]
        d_min = X[i,] - U[which.max(W[i,]),]
        d = sum(d_ij * d_ij) - sum(d_min * d_min)
        print(d)
        pi[j] * exp(-0.5 * d)
      }
      W[i,] = W[i,] / sum(W[i,])
    }
    
    #W = exp(log_W)
    d = mean(abs(W - old_W))
    print(paste("Iteration=", iter, ", D=", d, sep=""))
    if(d < converge.threshold) break()
    
    old_W = W
    
    #M Step
    # log_w_j = logDotExp(t_log_W, n_zeros)
    # U = exp(logDotExp(t_log_W, log_X) - c(log_w_j))
    # log_pi = log_w_j - log(N)
    w_sum = colSums(W)
    U = crossprod(W, X) / w_sum
    # for (j in 1:cluster.count) {
    #   U[j,] = colSums(X * W[,j]) / w_sum[j]
    # }
    pi = w_sum / N
    
    #print(log_pi)
    print(U)
  }
  
  return (list(W = W, centers = U, pi=pi))
}

```

```{r}
GMM_EM_Matrix = function(X, cluster.count, converge.threshold = 1e-7) {
  N = dim(X)[1]
  W = matrix(0, N, cluster.count)
  one_dk = matrix(1, dim(X)[2], cluster.count)
  one_nd = matrix(1, N, dim(X)[2])
  one_kn = matrix(1, cluster.count, N)
  one_n1 = matrix(1, N, 1)
  
  #Take kmean as the initial value for U and Pi
  kmeans_cluster = kmeans(X, cluster.count)
  
  U = kmeans_cluster$centers
  pi = kmeans_cluster$size / N
  for (j in 1:cluster.count){
    W[kmeans_cluster$cluster == j, j] = 1
  }
  
  old_W = W
  log_W = W

  for (iter in 1:10000){
    #E step
    H1 = (X * X) %*% one_dk
    H2 = one_nd %*% t(U * U)
    H3 = X %*% t(U)
    H  = - 1/2*(H1 + H2 - 2*H3)
    
    P = t(matrix(pi, cluster.count, N))
    E = exp(H) * P
    log_E = H + log(P)
    f = rep(0, N)
    for (i in 1:N){
      f[i] = logSumExp(log_E[i,])
    }
    log_F = matrix(f, N, cluster.count)
    log_W = log_E - log_F

    W = exp(log_W)
    
    d = mean(abs(W - old_W))
    print(paste("Iteration=", iter, ", D=", d, sep=""))
    if(d < converge.threshold) break()
    
    old_W = W
    
    #M Step
    # D = t(W) %*% X
    # G = t(W) %*% one_nd
    # U = D / G
    # 
    # pi = (t(W) %*% one_n1) / N
    w_sum = colSums(W)
    U = crossprod(W, X) / w_sum
    pi = w_sum / N

    
    print(U)
  }

  return (list(W = W, centers = U, pi=pi))
}
```

