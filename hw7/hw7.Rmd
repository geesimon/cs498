---
title: "Homework 7"
author: "CS 498, Spring 2018, Xiaoming Ji"
date: ''
output:
  html_document:
    toc: yes
  pdf_document: default
---


#Problem 1
**EM Topic models** The UCI Machine Learning dataset repository hosts several datasets recording word counts for documents here. You will use the NIPS dataset. You will find (a) a table of word counts per document and (b) a vocabulary list for this dataset at the link. You must implement the multinomial mixture of topics model.

```{r}
#Load data
header = readLines("docword.nips.txt", n = 2)
DocCount = as.integer(header[1])
VocabSize = as.integer(header[2])
df = read.table("docword.nips.txt", skip=3)

DocVectors = matrix(0, DocCount, VocabSize)
for (i in 1:(dim(df)[1])){
  DocVectors[df[i, 1],df[i, 2]] = df[i, 3]
}
```

- Cluster this to 30 topics, using a simple mixture of multinomial topic model.

We have,
$$
\omega_{i,j}=\frac{\left[\prod_{k} p_{j,k}^{x_{i,k}}\right]\pi_j}{ \sum_{l} \left[\prod_{k} p_{l,k}^{x_{i,k}}\right]\pi_l}
$$
We transform this equation to logsumexp form to benefit calculation
$$
\log(\omega_{i,j})=\sum_{k}(x_{i,k}\log p_{j,k}) + \log\pi_j - LogSumExp_l(\sum_{k}(x_{i,k}\log p_{l,k}) + \log\pi_{l})
$$

```{r}
library(matrixStats)

topic_EM = function(docs, vocab.size, topic.count, converge.threshold = 1e-7){
  #Initialize P and Pi
  doc_count = dim(docs)[1]
  doc_length = rowSums(docs)
  P  = matrix(0, topic.count, vocab.size)
  pi = rep(0, topic.count)
  W = matrix(0, doc_count, topic.count)
  old_W  = W
  log_W = W
  
  #Randomly sample docs
  i_doc = sample(1:doc_count, doc_count)
  
  bat_size = round(doc_count / topic.count)
  indexes = c(seq(from = 1, to = doc_count, by = bat_size), doc_count + 1)
  
  for (j in 1:topic.count) {
    i_bat = i_doc[indexes[j]:(indexes[j + 1] - 1)]
    
    #count words and add smoothing
    words = colSums(docs[i_bat,])
    
    #Calculate P and Pi
    P[j,]  = words / sum(words)
    pi[j] = length(i_bat) / doc_count
  }

  #Iterately perform E and M steps
  for (iter in 1:1000) {
    #smooth P
    P = P + 1e-7
    P = P / rowSums(P)
    log_P = log(P)
    
    log_pi = log(pi)
    
    #E Step
    X_log_P = docs %*% t(log_P)
    for (i in 1:doc_count) {
      log_W[i, ] = X_log_P[i, ] + log_pi
      log_W[i, ] = log_W[i, ] - logSumExp(log_W[i, ])
    }

    #print(log_W[1,])
    W = exp(log_W)
    t_W = t(W)
    
    d = max(abs(W - old_W))
    if (interactive()) print(paste("Iteration=", iter, ", D=", d, sep=""))
    if(d < converge.threshold) break()
    
    old_W = W
    
    #M Step
    P = (t_W %*% docs) / c(t_W %*% doc_length)
    pi = colSums(W) / doc_count
  }
  
  return (list(P=P, pi=pi))
}

logDotExp = function(A, B) {
  max_a = max(A)
  max_b = max(B)
  exp_a = exp(A - max_a)
  exp_b = exp(B - max_b)

  C = log(exp_a %*% exp_b)
  
  return (C + max_a + max_b)
}

log_topic_EM = function(docs, vocab.size, topic.count, converge.threshold = 1e-7){
  #Initialize P and Pi
  doc_count = dim(docs)[1]
  doc_length = rowSums(docs)
  P  = matrix(0, topic.count, vocab.size)
  pi = rep(0, topic.count)
  log_W = matrix(-10000, doc_count, topic.count)
  old_log_W = log_W
  #converge.threshold = abs(log(converge.threshold, 10))
  
  #Randomly sample docs
  i_doc = sample(1:doc_count, doc_count)
  
  bat_size = round(doc_count / topic.count)
  indexes = c(seq(from = 1, to = doc_count, by = bat_size), doc_count + 1)
  
  for (j in 1:topic.count) {
    i_bat = i_doc[indexes[j]:(indexes[j + 1] - 1)]
    
    #count words and add smoothing
    words = colSums(docs[i_bat,]) + (0.01 / topic.count) #smoothing
    
    #Calculate P and Pi
    P[j,]  = words / sum(words)
    pi[j] = length(i_bat) / doc_count
  }

  log_P = log(P)
  log_pi = log(pi)

  #Iterately perform E and M steps
  log_docs = log(docs)
  log_docs[log_docs < -10000] = -10000
  doc_count_zeros = matrix(0, doc_count, 1)
  
  for (iter in 1:10000) {
    #E Step
    X_log_P = docs %*% t(log_P)
    for (i in 1:doc_count) {
      log_W[i, ] = X_log_P[i, ] + log_pi
      log_W[i, ] = log_W[i, ] - logSumExp(log_W[i, ])
    }

    t_log_W = t(log_W)

    d = max(abs(exp(log_W) - exp(old_log_W)))
    print(paste("Iteration=", iter, ", D=", d, sep=""))
    if(d < converge.threshold) break()

    old_log_W = log_W
    
    #M Step
    log_P = logDotExp(t_log_W, log_docs) - c(logDotExp(t_log_W, log(doc_length)))
    log_pi = logDotExp(t_log_W, doc_count_zeros) - log(doc_count)

    log_P[log_P < -10000] = -10000
  }
  
  return (list(P=exp(log_P), pi = exp(log_pi)))
}
```

```{r}
TopicCount = 30

r=topic_EM(DocVectors, VocabSize, TopicCount)
#log_r=log_topic_EM(DocVectors, VocabSize, TopicCount)
```

- Produce a graph showing, for each topic, the probability with which the topic is selected.
```{r}
plot(r$pi, xlab = "Cluster #", ylab = "P", ylim = c(0, 0.05),
     col = "dodgerblue", pch = 20, cex = 1.5)
```

- Produce a table showing, for each topic, the 10 words with the highest probability for that topic.
```{r}
top_10 = matrix(0, TopicCount, 10)
P = r$P

for (i in 1:TopicCount) {
  for (j in 1: 10) {
    top_10[i, j] = which.max(P[i,])
    P[i, top_10[i, j]] = 0
  }
}

print(top_10)
```


#Problem 2
**Image segmentation using EM** You can segment an image using a clustering method - each segment is the cluster center to which a pixel belongs. In this exercise, you will represent an image pixel by its r, g, and b values (so use color images!). Use the EM algorithm applied to the mixture of normal distribution model lectured in class to cluster image pixels, then segment the image by mapping each pixel to the cluster center with the highest value of the posterior probability for that pixel. You must implement the EM algorithm yourself (rather than using a package). Test images are here, and you should display results for all three of them. Till then, use any color image you care to.


```{r}
library(jpeg)

read_image = function(file.name){
  return (readJPEG(file.name))
}
```

```{r}
#img= read_image("RobertMixed03.jpg")
img= read_image("smallstrelitzia.jpg")
#img= read_image("smallsunset.jpg")
old_dim = dim(img)

plot(c(0, dim(img)[2]), c(0, dim(img)[1]), type="n")
rasterImage(img, 0, 0, dim(img)[2], dim(img)[1])
dim(img) = c(dim(img)[1] * dim(img)[2], 3)
c= kmeans(img, 20)

for (i in 1:length(c$cluster)){
  img[i,] = c$centers[c$cluster[i],]
}

dim(img) = old_dim


plot(c(0, dim(img)[2]), c(0, dim(img)[1]), type="n")
rasterImage(img, 0, 0, dim(img)[2], dim(img)[1])
```

- Segment each of the test images to 10, 20, and 50 segments. You should display these segmented images as images, where each pixel's color is replaced with the mean color of the closest segment

```{r}
GMM_EM = function(X, cluster.count, converge.threshold = 1e-4) {
  N = dim(X)[1]
  W = matrix(0, N, cluster.count)
  #Take kmean as the initial value for U and Pi
  kmeans_cluster = kmeans(X, cluster.count)
  
  U = kmeans_cluster$centers
  pi = kmeans_cluster$size / N

  old_W = W
  log_W = W
  
  for (iter in 1:10000) {
    log_pi = log(pi)
    #E Step
    for (j in 1:cluster.count) {
      D = t(X) - U[j,]
      log_W[,j] = (-0.5 * colSums(D * D)) + log_pi[j]
    }
    for (i in 1:N) {
      log_W[i, ] = log_W[i,] - logSumExp(log_W[i,])
    }

    W = exp(log_W)
    d = mean(abs(W - old_W))
    
    print(paste("Iteration=", iter, ", D=", d, sep=""))
    flush.console()
    if(d < converge.threshold) break()
    
    old_W = W
    
    #M Step
    w_sum = colSums(W)
    U = crossprod(W, X) / w_sum
    pi = w_sum / N
  }
  
  return (list(W = W, centers = U, pi=pi))
}


segment_image = function(filename, cluster.count) {
  img = readJPEG(filename)
  old_dim = dim(img)
  dim(img) = c(dim(img)[1] * dim(img)[2], 3)
  
  r = GMM_EM(img * 255, cluster.count)
  r$centers = r$centers / 255
  
  for (i in 1:dim(r$W)[1]){
    img[i,] = r$centers[which.max(r$W[i,]),]
  }
  dim(img) = old_dim

  return (img)
}
```


```{r}
image_names = c("RobertMixed03.jpg", "smallstrelitzia.jpg", "smallsunset.jpg")
clusters = c(10, 20, 50)

for (name in image_names) {
  for (c in clusters) {
    img = segment_image(name, c)
    plot(c(0, dim(img)[2]), c(0, dim(img)[1]), type="n", 
         main=paste(name, ", Segments=",c, sep=""),
         xlab = "X", ylab = "Y")
    rasterImage(img, 0, 0, dim(img)[2], dim(img)[1]) 
  }
}
```

- We will identify one special test image. You should segment this to 20 segments using five different start points, and display the result for each case. Is there much variation in the result?

